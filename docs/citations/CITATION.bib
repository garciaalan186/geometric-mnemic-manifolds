@techreport{garcia2025geometric,
  author       = {Garcia, Alan},
  title        = {{Geometric Mnemic Manifolds: A Foveated Architecture
                   for Autonoetic Memory in LLMs}},
  month        = dec,
  year         = 2025,
  type         = {Position Paper},
  institution  = {Independent Research},
  doi          = {10.5281/zenodo.17849006},
  url          = {https://github.com/garciaalan186/geometric-mnemic-manifolds}
}

@misc{garcia2025geometric_arxiv,
  author       = {Garcia, Alan},
  title        = {{Geometric Mnemic Manifolds: A Foveated Architecture
                   for Autonoetic Memory in LLMs}},
  howpublished = {Position Paper, arXiv preprint},
  year         = 2025,
  month        = dec,
  eprint       = {XXXX.XXXXX},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  keywords     = {Artificial Intelligence, Memory Systems, Geometric Topology,
                  Autonoetic Memory, Vector Databases, RAG Systems},
  url          = {https://github.com/garciaalan186/geometric-mnemic-manifolds},
  abstract     = {We propose a novel architecture for simulating the functional
                  dynamics of autonoetic memory in AI systems, departing from the
                  industry standard of stochastic Vector Databases. We introduce
                  the Geometric Mnemic Manifold, a system where a Recursive
                  Reasoning Kernel (RRK) acts as a fluid reasoning engine,
                  offloading long-term memory to a distributed graph of immutable
                  Engrams. Unlike standard RAG systems which optimize solely for
                  semantic relevance, this architecture organizes engrams along a
                  deterministic, low-discrepancy trajectory utilizing Kronecker
                  sequences on the hypersphere. By utilizing Hierarchical Radial
                  Connectivity coupled with logarithmic radial expansion, the
                  system achieves a mathematically rigorous Foveated Memory effect.
                  This exponential decay of information density allows for
                  Logarithmic Semantic Traversal (O(log N)) with Constant Time
                  Addressing (O(1)), mimicking the biological efficiency of human
                  memory consolidation while solving the "Cold Start" latency
                  problem inherent in graph-based indexing.}
}
