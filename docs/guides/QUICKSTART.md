# Geometric Mnemic Manifolds - Quick Start Guide

## ğŸ¯ What You Have

1. **gmm_explainer.html** - Interactive explanation with clear analogies
2. **gmm_prototype.py** - Working GMM implementation
3. **benchmark_suite.py** - Performance testing suite
4. **demo.py** - Interactive demonstrations
5. **README.md** - Complete documentation

## âš¡ 30-Second Start

### Step 1: View the Explainer
```bash
# Just open in your browser - no installation needed!
open gmm_explainer.html
```

### Step 2: Run the Demo
```bash
pip install numpy matplotlib
python demo.py
```
Select option 8 to run all demonstrations.

## ğŸ“– What Is This?

The **Geometric Mnemic Manifold** is a novel architecture for AI memory that:

- Organizes memories on an **exponential spiral** (like Fermat's spiral meets time)
- Uses **3 hierarchical layers** (raw â†’ patterns â†’ abstracts)
- Achieves **O(1) retrieval time** regardless of memory size
- Enables **zero cold-start** (no index loading needed)
- Supports **ephemeral clones** (talk to your past self!)

Think of it as: *What if AI memory worked like human memory instead of like a database?*

## ğŸ“ Learning Path

### For AI Enthusiasts
1. Read the **interactive explainer** (gmm_explainer.html)
2. Run the **interactive demo** (demo.py)
3. Check out the visualizations

### For Researchers
1. Read the **full paper** (geometric_mnemic_manifolds_v22.tex)
2. Study the **prototype code** (gmm_prototype.py)
3. Run the **benchmarks** (benchmark_suite.py)
4. Examine the complexity analysis

### For Developers
1. Skim the **README** for architecture overview
2. Check **gmm_prototype.py** for implementation details
3. Modify the code to add real embeddings:
   ```python
   from sentence_transformers import SentenceTransformer
   model = SentenceTransformer('all-MiniLM-L6-v2')
   embedding = model.encode(text)
   ```

## ğŸ”¬ Running Experiments

### Basic Prototype Demo
```bash
python gmm_prototype.py
```
This will:
- Create 500 synthetic memories
- Build 3-layer hierarchy
- Demonstrate O(1) retrieval
- Export visualization data

### Full Benchmark Suite
```bash
python benchmark_suite.py
```
This will:
- Test at depths: 100, 500, 1K, 5K memories
- Compare GMM vs HNSW
- Generate performance plots
- Save results to `./benchmark_results/`

Expected runtime: 2-5 minutes

### Interactive Demo
```bash
python demo.py
```
Choose from 7 demonstrations:
1. Spiral geometry positioning
2. Hierarchical layer construction
3. Memory retrieval algorithm
4. Foveal ring concept
5. Temporal foveation effect
6. Zero cold-start comparison
7. Interactive query interface

## ğŸ¨ Key Visualizations

The explainer includes 4 interactive visualizations:

1. **Spiral Canvas**: See memories arranged on exponential Kronecker spiral
2. **Hierarchy Diagram**: Understand 3-layer skip-list structure
3. **Query Animation**: Watch hierarchical search in action
4. **Benchmark Plot**: GMM vs HNSW performance comparison

All are interactive - adjust parameters and see results in real-time!

## ğŸ—ï¸ Architecture at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Recursive Reasoning Kernel (RRK)      â”‚
â”‚  Small, fluid intelligence engine       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Neural Bus (IPC)
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Geometric Manifold              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 2: Abstracts  [N/1024 nodes]     â”‚
â”‚ Layer 1: Patterns   [N/64 nodes]       â”‚
â”‚ Layer 0: Raw        [N nodes]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Organized on Kronecker Spiral
     r(k) = e^(Î»k), Î¸(k) = 2Ï€{kâˆšprimes}
```

## ğŸ’¡ Key Innovations

1. **Time as Geometry**: Not metadata, but a physical coordinate
2. **Foveated Memory**: Dense recent, sparse distant (like vision)
3. **O(1) Addressing**: Mathematical calculation, not index lookup
4. **Active Retrieval**: Get computational agents, not text strings
5. **Zero Index**: Serverless instantiation

## ğŸ“Š Typical Performance

From benchmark suite (your results may vary):

| Memories | GMM Time | HNSW Time | Speedup |
|----------|----------|-----------|---------|
| 100      | ~5ms     | ~7ms      | 1.4x    |
| 1,000    | ~6ms     | ~16ms     | 2.7x    |
| 5,000    | ~6ms     | ~24ms     | 4.0x    |
| 10,000   | ~6ms     | ~30ms     | 5.0x    |

Notice: GMM stays constant, HNSW grows logarithmically!

## ğŸ”§ Next Steps

### Add Real Embeddings
```bash
pip install sentence-transformers
# Then modify gmm_prototype.py to use real models
```

### Train an RRK
```python
# Implement epistemic gap detection
L_signal = -E[(1-m) log p(signal|q,W) + m log(1-p(signal|q,W))]
# Train small model (Phi-3, Qwen) with this loss
```

### Scale Up
- Use actual vector storage (Redis, Qdrant)
- Implement distributed sharding
- Add multi-modal engrams (images, audio)

## ğŸ¤” Common Questions

**Q: Is this just fancy RAG?**
A: No! RAG retrieves text. GMM retrieves computational agents (past selves).

**Q: Why not just use a bigger context window?**
A: Computational cost scales quadratically. This stays constant.

**Q: What about training cost?**
A: RRK is small (3B params). Engrams are frozen (no gradients needed).

**Q: Can I use this in production?**
A: This is a research prototype. Add production DB, auth, monitoring, etc.

## ğŸ“š Further Reading

- Original paper: `geometric_mnemic_manifolds_v22.tex`
- Full README: `README.md`
- Tulving (1985): Episodic memory and autonoesis
- Park et al. (2023): Generative Agents (for comparison)

## ğŸ› Troubleshooting

**Demo won't run:**
```bash
pip install --upgrade numpy matplotlib
```

**Plots don't appear:**
```bash
# On macOS
brew install python-tk
# On Ubuntu
apt-get install python3-tk
```

**Want to modify:**
- All code is extensively commented
- Start with `demo.py` for examples
- `gmm_prototype.py` has full implementation

## ğŸ‰ Have Fun!

The best way to understand this is to:
1. Play with the interactive explainer
2. Run the demos and watch the console
3. Modify the code and see what breaks
4. Read the paper for theoretical depth

Remember: "What I cannot create, I do not understand."

---

**Questions?** Check README.md or open the explainer!
